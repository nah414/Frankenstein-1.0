# FRANKENSTEIN 1.0 - Eye of Sauron Configuration
# Phase 4: Master Orchestrator Agent
# Model: Llama 3.2 3B Instruct Q4_K_M (served via Ollama)
# Hardware: Dell i3 8th Gen, 4 cores, 8GB RAM

sauron:
  version: "1.2.0"

  model:
    name: "llama3.2:3b"
    provider: "ollama"
    quantization: "Q4_K_M"
    parameters: "3B"

  inference:
    # Profile B — Chat Primary (default when quantum engine is idle)
    num_ctx: 2048
    num_thread: 2
    temperature: 0.5
    top_p: 0.9
    repeat_penalty: 1.18
    repeat_last_n: 64
    num_predict: 300
    keep_alive: "3m"

  inference_quantum:
    # Profile A — Quantum Active (auto-selected when quantum engine is running)
    num_ctx: 1024
    num_thread: 2
    temperature: 0.5
    top_p: 0.9
    repeat_penalty: 1.18
    repeat_last_n: 64
    num_predict: 180
    keep_alive: "90s"

  resource_budget:
    max_ram_gb: 2.5            # Llama 3.2 3B model (~2.0GB) + 0.5GB inference buffer
    max_cpu_inference: 50      # Target CPU % during active inference
    max_cpu_idle: 5            # Target CPU % when loaded but waiting
    hard_cpu_limit: 80         # Emergency stop threshold (matches SAFETY)

  throttle_thresholds:
    soft_cpu: 65               # Begin reducing context/output
    soft_ram: 65
    hard_cpu: 75               # Switch to Profile A, enforce short outputs
    hard_ram: 75               # Max RAM ceiling
    emergency_cpu: 80          # Unload model + block inference
    emergency_ram: 90          # Spike tolerance ceiling before emergency unload

  behavior:
    lazy_load: true            # NEVER load at terminal startup
    idle_timeout_sec: 300      # 5 min inactivity → unload model
    max_conversation_turns: 8
    stream_by_default: true    # Stream output to terminal
    max_response_tokens_default: 300
    max_response_tokens_quantum: 180

  paths:
    log_file: "~/.frankenstein/logs/sauron.log"
